## **Contexte du projet**  
Nous travaillons pour **SocialMetrics AI**, une entreprise fictive spÃ©cialisÃ©e dans lâ€™analyse des donnÃ©es issues des rÃ©seaux sociaux. Notre client, **Daunale Treupe**, souhaite mettre en place un service permettant dâ€™Ã©valuer le sentiment des tweets en fonction de leur contenu.  

### **Objectifs du projet**  
1. DÃ©velopper une **API Flask** pour traiter les tweets en temps rÃ©el et renvoyer une analyse de sentiment (positif ou nÃ©gatif).  
2. Utiliser un **modÃ¨le de machine learning basÃ© sur la rÃ©gression logistique** pour effectuer ces prÃ©dictions.  
3. **RÃ©entraÃ®ner rÃ©guliÃ¨rement** le modÃ¨le avec de nouvelles donnÃ©es pour amÃ©liorer sa prÃ©cision au fil du temps.  
4. Ã‰valuer la performance du modÃ¨le Ã  lâ€™aide dâ€™**indicateurs clÃ©s** (prÃ©cision, rappel, F1-score) et de **matrices de confusion**.  


## **Acquisition des donnÃ©es**  
Lâ€™entraÃ®nement dâ€™un bon modÃ¨le nÃ©cessite une **grande quantitÃ© de donnÃ©es** pour quâ€™il puisse apprendre efficacement. Nous avons rÃ©cupÃ©rÃ© **environ 80 000 tweets** grÃ¢ce Ã  **Kaggle** (merci Kaggle de nous avoir facilitÃ© cette Ã©tape !) :  
- **40 000 tweets** provenant de Twitter  
- **40 000 commentaires** issus de Reddit  

Ces donnÃ©es nous serviront Ã  entraÃ®ner et tester notre modÃ¨le dâ€™analyse de sentiment.  

### **Utilisation progressive des donnÃ©es**  
Pour la **premiÃ¨re vague de tests**, nous allons utiliser **40 000 tweets uniquement**, afin dâ€™avoir une base de dÃ©part sans surcharge de calcul. Lâ€™idÃ©e est dâ€™utiliser **seulement 1/40k** pour faire des premiers tests et valider la faisabilitÃ© du modÃ¨le avant de lâ€™affiner avec plus de donnÃ©es.  


## **Pourquoi bien diviser les donnÃ©es ?**  
Un bon modÃ¨le dâ€™IA repose dâ€™abord sur **beaucoup de donnÃ©es dâ€™entraÃ®nement**, mais ce nâ€™est pas suffisant. Il faut aussi sâ€™assurer quâ€™il **gÃ©nÃ©ralise bien** et ne se contente pas de "mÃ©moriser" les tweets quâ€™il a dÃ©jÃ  vus. Câ€™est lÃ  quâ€™intervient la division des donnÃ©es.  

### **Nettoyage des donnÃ©es**  
Avant de les utiliser, les donnÃ©es doivent Ãªtre **nettoyÃ©es** pour supprimer :  
âœ… **Les mots inutiles** (a, the, andâ€¦) qui nâ€™apportent rien au sens global du tweet.  
âœ… **Les caractÃ¨res spÃ©ciaux**, les hashtags, les emojis, et autres Ã©lÃ©ments qui peuvent biaiser lâ€™analyse.  
âœ… **Les doublons**, afin dâ€™Ã©viter que le modÃ¨le surapprenne certaines expressions trop prÃ©sentes.  

### **Pourquoi diviser les donnÃ©es en plusieurs ensembles ?**  
La division des donnÃ©es en plusieurs groupes permet de :  

1ï¸âƒ£ **EntraÃ®ner le modÃ¨le** :  
- Le modÃ¨le apprend les **motifs et structures des tweets** pour diffÃ©rencier les sentiments positifs et nÃ©gatifs.  
- Ex : Un tweet comme *"J'adore cette journÃ©e ! ğŸ˜Š"* sera Ã©tiquetÃ© **positif**, et un tweet comme *"Cette journÃ©e est horrible ğŸ˜¡"* sera **nÃ©gatif**.  

2ï¸âƒ£ **Ã‰valuer le modÃ¨le** :  
- AprÃ¨s l'entraÃ®nement, il faut tester le modÃ¨le sur des tweets **qu'il n'a jamais vus** pour vÃ©rifier qu'il ne fait pas dâ€™erreurs.  
- Ex : Si on donne le tweet *"Ce produit est trop cher pour sa qualitÃ©"* et que le modÃ¨le le classe **positif**, cela signifie quâ€™il ne comprend pas bien le contexte.  

3ï¸âƒ£ **Ã‰viter le surapprentissage (overfitting)** :  
- Si on entraÃ®ne le modÃ¨le **uniquement sur les tweets quâ€™il connaÃ®t**, il risque dâ€™apprendre "par cÅ“ur" au lieu de **comprendre rÃ©ellement**.  
- Ex : Un Ã©tudiant qui mÃ©morise toutes les rÃ©ponses dâ€™un examen dâ€™entraÃ®nement mais qui Ã©choue quand on lui pose une nouvelle question.  


## **Comment diviser les donnÃ©es ?**  
Pour entraÃ®ner et tester notre modÃ¨le, nous devons diviser notre dataset en **plusieurs ensembles** :  
ğŸ”¹ **DonnÃ©es dâ€™entraÃ®nement** (Training set) : utilisÃ©es pour apprendre.  
ğŸ”¹ **DonnÃ©es de test** (Test set) : utilisÃ©es pour Ã©valuer le modÃ¨le.  
ğŸ”¹ **DonnÃ©es pour le rÃ©entraÃ®nement** : utilisÃ©es plus tard pour amÃ©liorer le modÃ¨le.  

### **Choix du pourcentage de division**  
Il existe plusieurs stratÃ©gies de rÃ©partition des donnÃ©es , chacune avec ses avantages et inconvÃ©nients. 


### **1ï¸âƒ£ 70% entraÃ®nement / 30% test** (*mÃ©thode classique*)
---
ğŸ“Œ **Principe** :  
- 70% des tweets servent Ã  entraÃ®ner le modÃ¨le.  
- 30% sont gardÃ©s pour tester sa performance.  

âœ… **Avantages** :  
âœ” Bonne rÃ©partition entre apprentissage et Ã©valuation.  
âœ” Permet une estimation fiable des performances du modÃ¨le.  
âœ” Convient aux datasets de taille moyenne.  

âŒ **InconvÃ©nients** :  
âœ˜ Peut Ãªtre insuffisant pour un entraÃ®nement optimal si le dataset est trop petit.  
âœ˜ Une rÃ©partition fixe ne prend pas en compte les Ã©volutions des donnÃ©es.  

ğŸ” **Exemple** :  
Imaginons un modÃ¨le de classification de sentiments sur Twitter. Si nous avons **10 000 tweets**, alors **7 000** serviront Ã  entraÃ®ner le modÃ¨le et **3 000** Ã  le tester. Si les tendances sur Twitter changent rapidement, les **30% de test peuvent devenir obsolÃ¨tes** aprÃ¨s un certain temps.  


### **2ï¸âƒ£ 80% entraÃ®nement / 20% test** (*si on a beaucoup de donnÃ©es*)  
---
ğŸ“Œ **Principe** :  
- On maximise lâ€™apprentissage (80%).  
- On garde un test fiable (20%).  

âœ… **Avantages** :  
âœ” Plus de donnÃ©es pour lâ€™apprentissage, donc un modÃ¨le potentiellement plus performant.  
âœ” Toujours une bonne Ã©valuation avec 20% de test.  

âŒ **InconvÃ©nients** :  
âœ˜ Moins de donnÃ©es pour tester peut donner une Ã©valuation lÃ©gÃ¨rement moins prÃ©cise.  
âœ˜ Risque de surentraÃ®nement (overfitting) si les 80% sont trop homogÃ¨nes.  

ğŸ” **Exemple** :  
Si nous avons **100 000 tweets**, **80 000** seront utilisÃ©s pour entraÃ®ner le modÃ¨le et **20 000** pour lâ€™Ã©valuer. Cette approche est particuliÃ¨rement utile si nous avons des donnÃ©es variÃ©es et en grande quantitÃ©.  


### **3ï¸âƒ£ 90% entraÃ®nement / 10% test** (*optimisation maximale de lâ€™apprentissage*)  
---
ğŸ“Œ **Principe** :  
- 90% des donnÃ©es pour entraÃ®ner le modÃ¨le.  
- 10% seulement pour Ã©valuer.  

âœ… **Avantages** :  
âœ” IdÃ©al si on a un grand dataset et quâ€™on veut maximiser lâ€™apprentissage.  
âœ” Peut amÃ©liorer la prÃ©cision du modÃ¨le sur de nouvelles donnÃ©es.  

âŒ **InconvÃ©nients** :  
âœ˜ Peu de donnÃ©es pour tester â†’ risque dâ€™une Ã©valuation peu fiable.  
âœ˜ Risque dâ€™overfitting car le modÃ¨le est trop optimisÃ© pour lâ€™ensemble dâ€™entraÃ®nement.  

ğŸ” **Exemple** :  
Si nous avons **1 million de tweets**, nous utilisons **900 000** pour lâ€™entraÃ®nement et seulement **100 000** pour le test. Cela permet dâ€™entraÃ®ner un modÃ¨le puissant, mais nous avons moins de recul sur ses performances rÃ©elles.  


### **4ï¸âƒ£ 66% entraÃ®nement / 33% pour rÃ©entraÃ®nement** (*notre choix*)  
---
ğŸ“Œ **Principe** :  
- 66% des donnÃ©es pour lâ€™entraÃ®nement initial.  
- 33% des donnÃ©es mises de cÃ´tÃ© pour Ãªtre utilisÃ©es plus tard comme nouvelles donnÃ©es de rÃ©entraÃ®nement.  

âœ… **Avantages** :  
âœ” Permet dâ€™entraÃ®ner le modÃ¨le sur des donnÃ©es rÃ©centes.  
âœ” Offre la possibilitÃ© de rÃ©entraÃ®nement avec des nouvelles donnÃ©es au fil du temps.  
âœ” Sâ€™adapte aux tendances changeantes (ex. nouvelles expressions sur Twitter).  

âŒ **InconvÃ©nients** :  
âœ˜ Au dÃ©part, on teste avec les mÃªmes donnÃ©es dâ€™entraÃ®nement.  
âœ˜ On doit attendre pour utiliser les donnÃ©es mises de cÃ´tÃ©, ce qui peut ralentir lâ€™amÃ©lioration immÃ©diate du modÃ¨le.  

ğŸ” **Exemple** :  
- On **cache 10 000 tweets**comme s'ils n'existaient pas.
- On utilise les **30 000 tweets restants** pour entraÃ®ner et valider le modÃ¨le.
- Une fois satisfait, on **rÃ©intÃ¨gre les 10 000 tweets cachÃ©s** pour tester si le modÃ¨le gÃ©nÃ©ralise bien sur ces nouvelles donnÃ©es.


### **5ï¸âƒ£ Validation croisÃ©e (Cross-Validation)**  
---
En plus de la division classique, nous pouvons utiliser une **validation croisÃ©e** pour mieux Ã©valuer notre modÃ¨le.  

ğŸ“Œ **Principe** :  
On divise les donnÃ©es en plusieurs **sous-ensembles** et on entraÃ®ne plusieurs modÃ¨les en utilisant chaque sous-ensemble comme test Ã  tour de rÃ´le.  

âœ… **Avantages** :  
âœ” Permet une meilleure Ã©valuation du modÃ¨le en utilisant toutes les donnÃ©es.  
âœ” RÃ©duit les biais de division fixe (ex. si les donnÃ©es sont mal rÃ©parties).  
âœ” Utile surtout pour les petits datasets.  

âŒ **InconvÃ©nients** :  
âœ˜ Plus coÃ»teux en calculs â†’ prend plus de temps.  
âœ˜ Pas toujours nÃ©cessaire si on a beaucoup de donnÃ©es.  

ğŸ” **Exemple** :  
Avec une validation croisÃ©e **5-fold**, on divise les **10 000 tweets** en **5 groupes de 2 000 tweets**. On entraÃ®ne 5 modÃ¨les, chacun utilisant 4 groupes pour lâ€™entraÃ®nement et 1 groupe pour le test, puis on fait la moyenne des performances.  
 

### **tableau recapitulatif : quel choix adopter ?**  

| MÃ©thode                  | Avantages | InconvÃ©nients | IdÃ©al pour |
|-------------------------|-----------|--------------|------------|
| **70% entraÃ®nement / 30% test** | Bon Ã©quilibre entre apprentissage et test | Risque de ne pas capturer les Ã©volutions des donnÃ©es | Datasets moyens |
| **80% entraÃ®nement / 20% test** | Plus dâ€™apprentissage sans trop rÃ©duire lâ€™Ã©valuation | LÃ©gÃ¨rement moins fiable pour le test | Datasets volumineux |
| **90% entraÃ®nement / 10% test** | Maximisation de lâ€™apprentissage | Test moins fiable, risque dâ€™overfitting | TrÃ¨s grands datasets |
| **66% entraÃ®nement / 33% rÃ©entraÃ®nement** | Permet une adaptation continue du modÃ¨le | NÃ©cessite un suivi rÃ©gulier | ModÃ¨les Ã©volutifs (ex. Twitter) |
| **Validation croisÃ©e** | Meilleure Ã©valuation | Plus long Ã  calculer | Petits datasets |

ğŸš€ **Dans notre cas, nous avons choisi la rÃ©partition 66% / 33%** pour garantir un modÃ¨le qui **sâ€™amÃ©liore dans le temps** et suit les Ã©volutions du langage sur Twitter.


<!-- 

## **RÃ©sumÃ© de la rÃ©flexion**  (02/02/2025)
ğŸ”¹ Nous avons rÃ©cupÃ©rÃ© **~80 000 tweets** (Twitter + Reddit).  
ğŸ”¹ Nous utilisons **40 000 tweets** pour la premiÃ¨re phase de tests.  
ğŸ”¹ Nous devons **nettoyer les donnÃ©es** (supprimer les mots inutiles, emojis, etc.).  
ğŸ”¹ La division des donnÃ©es permet de **mieux entraÃ®ner et tester** le modÃ¨le.  
ğŸ”¹ Nous avons choisi **66% pour lâ€™entraÃ®nement** et **33% pour le rÃ©entraÃ®nement** futur.  
<!-- ğŸ”¹ Lâ€™objectif final est dâ€™avoir une IA **prÃ©cise**, **gÃ©nÃ©ralisable** et **capable dâ€™Ã©voluer avec le temps**.   -->

-----
## **Comment diviser les donnÃ©es ?**  

Pour entraÃ®ner et tester notre modÃ¨le, nous devons diviser notre dataset en **plusieurs ensembles** :  
ğŸ”¹ **DonnÃ©es dâ€™entraÃ®nement (Training set)** : utilisÃ©es pour apprendre.  
ğŸ”¹ **DonnÃ©es de test (Test set)** : utilisÃ©es pour Ã©valuer le modÃ¨le.  
ğŸ”¹ **DonnÃ©es pour le rÃ©entraÃ®nement** : utilisÃ©es plus tard pour amÃ©liorer le modÃ¨le.  

---

---


ğŸ”¹ **Si on veut un modÃ¨le classique â†’ 70/30**  
ğŸ”¹ **Si on a beaucoup de donnÃ©es â†’ 80/20**  
ğŸ”¹ **Si on veut un modÃ¨le trÃ¨s prÃ©cis â†’ 90/10**  
ğŸ”¹ **Si on veut rÃ©entraÃ®ner rÃ©guliÃ¨rement â†’ 66/33**  
ğŸ”¹ **Si on a un petit dataset â†’ Validation croisÃ©e**  
 -->

prochaine etape reflechire a comment integre tous ca techniqument
---